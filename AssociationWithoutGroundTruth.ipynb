{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e506b-370f-40da-acf7-61f378570875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.fairness import bias_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346c5906-5950-4908-bbbf-0dbdbc6db5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.fairness.bias_metrics import AssociationWithoutGroundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1792d1-906c-4557-a0b4-57349d0be112",
   "metadata": {},
   "source": [
    "[AssociationWithoutGroundTruth](http://docs.allennlp.org/main/api/fairness/bias_metrics/#associationwithoutgroundtruth) measures model biases in the absence of ground truth. \n",
    "It does so by computing one-vs-all or pairwise association gaps using statistical measures \n",
    "like `nPMIxy`, `nPMIy`, `PMI^2`, and `PMI`, which are capable of capturing labels across a \n",
    "range of marginal frequencies. A gap of nearly 0 implies less bias on the basis of Association \n",
    "the Absence of Ground Truth.\n",
    "`AssociationWithoutGroundTruth` can be used like any other `Metric` in the library, and it is supported in distributed settings. Here is an example code snippet for using this metric with your own models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0ffd2-d567-4071-a4e0-cd784711f17a",
   "metadata": {},
   "source": [
    "@Metric.register(\"association_without_ground_truth\")\n",
    "class AssociationWithoutGroundTruth(Metric):\n",
    " | def __init__(\n",
    " |     self,\n",
    " |     num_classes: int,\n",
    " |     num_protected_variable_labels: int,\n",
    " |     association_metric: str = \"npmixy\",\n",
    " |     gap_type: str = \"ova\"\n",
    " | ) -> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5e826-3938-472a-a6e5-061c1fe634c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "self._npmixy = AssociationWithoutGroundTruth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02017a2-76fc-434e-abee-cd5cbe1111fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, *args, **kwargs):\n",
    "    # Accumulate metric over batches\n",
    "    self._npmixy(predicted_labels, protected_variable_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd72475-3765-4e95-b7dc-a62e6e550904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YourModel(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3ea80-4f90-4026-a70f-b7ca3b589e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final values of metric after all batches have been processed\n",
    "print(model._npmixy.get_metric())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datametrics",
   "language": "python",
   "name": "datametrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
