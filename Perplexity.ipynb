{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ddfdc9-d6a6-46f4-af7d-2ce3f1aa8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f659385f-0c6b-443d-afb6-bbc1bddce019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33afd47c-db67-48ea-b559-ac16e6393a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e7565b5-ba5a-414f-a6f3-ccc73d1a3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplex_models_data(models,datasets):\n",
    "    combos= list(itertools.product(models, datasets))\n",
    "    print(\"There are \" + str(len(list(combos))) + \" combinations of models and datasets:\")\n",
    "    print(combos)\n",
    "    loadedmodels=[]\n",
    "    for m,d in combos:\n",
    "        if m in loadedmodels:\n",
    "            #figure out how to do this\n",
    "            print(str(m) + \" was already loaded!\")\n",
    "        str(m)+\"_tokenizer\" = AutoTokenizer.from_pretrained(m)\n",
    "        str(m)+ \"_model\" = AutoModelForMaskedLM.from_pretrained(m)\n",
    "        \n",
    "        print(str(m) + \" model loaded!\")\n",
    "        test = load_dataset(d, split='train', streaming=True)\n",
    "        print(str(d) + \" dataset loaded!\")\n",
    "        data_head = test.take(5000)\n",
    "        text= [l['text'] for l in list(data_head)]\n",
    "        encodings = tokenizer('\\n\\n'.join(text), return_tensors='pt')\n",
    "        max_length = model.config.max_position_embeddings\n",
    "        #From https://huggingface.co/transformers/perplexity.html\n",
    "        stride = 512\n",
    "        lls = []\n",
    "        for i in range(0, encodings.input_ids.size(1), stride):\n",
    "            begin_loc = max(i + stride - max_length, 0)\n",
    "            end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "            trg_len = end_loc - i    # may be different from stride on last loop\n",
    "            input_ids = encodings.input_ids[:,begin_loc:end_loc].to(device)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:,:-trg_len] = -100\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "                log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "            lls.append(log_likelihood)\n",
    "\n",
    "        ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "        print(\"The perplexity of the \" + str(m) + \" model with the \" + str(d) + \" dataset is \" + str(ppl.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ab1b53a-3443-4f9b-9662-a131f1360327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 combinations of models and datasets:\n",
      "[('bert-base-uncased', 'sentiment140')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased model loaded!\n",
      "sentiment140 dataset loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (103137 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the bert-base-uncased model with the sentiment140 dataset is 1.7406984567642212\n"
     ]
    }
   ],
   "source": [
    "perplex_models_data([\"bert-base-uncased\"], [\"sentiment140\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf508891-2d61-4ab6-8d56-ffc270996855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76e447-f285-4b06-ae4c-f1fcd3e134e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first 50 from openwebtext: tensor(2.0917)\n",
    "wiki sample: tensor(1.5268)\n",
    "500 lines of poetry (poem_sentiment) : tensor(3.2618)\n",
    "500 tweets from sentiment140: tensor(1.6752)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47822e1-c233-4560-b7db-a915efd46a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "base_url = \"https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/\"\n",
    "data_files = {\"train\": base_url + \"wikipedia-train.parquet\"}\n",
    "wiki = load_dataset(\"parquet\", data_files=data_files, split=\"train\", streaming=True)\n",
    "print(next(iter(wiki)))\n",
    "# {'title': 'Yangliuqing', 'text': 'Yangliuqing () is a market town in Xiqing District...'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f57922-e637-4aa8-8076-3b85d017a210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a002a9a-ac0f-4ab0-94ea-42b86b85ca59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2b0df-1465-499f-b3ca-14aa4e52dad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11c593-faca-4710-bd5c-6b41530bf2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datametrics",
   "language": "python",
   "name": "datametrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
