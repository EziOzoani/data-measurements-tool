{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ddfdc9-d6a6-46f4-af7d-2ce3f1aa8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f659385f-0c6b-443d-afb6-bbc1bddce019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33afd47c-db67-48ea-b559-ac16e6393a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1592d3f8-7fae-4870-981e-46452b97f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= load_dataset(\"glue\",\"ax\", split = \"test\", streaming= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "271a8bf4-0d5c-4492-9659-7b51b129da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mods= [\"bert-base-uncased\", \"t5-small\"]\n",
    "\n",
    "datas= [[\"oscar\", \"unshuffled_deduplicated_en\", \"train\", True,5],\n",
    "        [\"imdb\",\"plain_text\", \"test\", False ,5],\n",
    "        [\"poem_sentiment\",\"plain_text\", \"test\", True ,5],        \n",
    "        [\"c4\", \"en\", \"train\", True, 5]      \n",
    "       ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636f9b6-e6a9-49e6-826c-ddb567b65787",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplex_model_data(m_name =\"bert-base-uncased\", d_name =\"oscar\", d_option=\"unshuffled_deduplicated_en\", d_split = \"train\", d_streaming = True, d_size=5 )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "713ae324-1b1d-47e1-a1c6-1dce1b0fdd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplex_model_data(**kwargs):\n",
    "    modlist=[]\n",
    "    maskedLM= [\"bert-base-uncased\"]\n",
    "    maskedHead= [\"t5-small\"]\n",
    "    tok = AutoTokenizer.from_pretrained(kwargs['m_name'])\n",
    "    #TODO: fix the redundant loading of models!\n",
    "    if kwargs['m_name'] in modlist: \n",
    "        next \n",
    "    elif kwargs['m_name'] in maskedHead:\n",
    "        model = AutoModelWithLMHead.from_pretrained(kwargs['m_name'])\n",
    "    else: \n",
    "        model = AutoModelForMaskedLM.from_pretrained(kwargs['m_name'])\n",
    "        print(str(kwargs['m_name']) + \" model loaded!\")\n",
    "        modlist.append(kwargs['m_name'])\n",
    "    data= load_dataset(kwargs['d_name'], kwargs['d_option'], split= kwargs['d_split'], streaming = kwargs['d_streaming'])\n",
    "    print(str(kwargs['d_name']) + \" dataset loaded!\")\n",
    "    if kwargs['d_streaming'] == True: \n",
    "        data_head = data.take(kwargs['d_size'])\n",
    "        try: \n",
    "            text= [l['text'] for l in list(data_head)]\n",
    "        except:\n",
    "            feature = [d for d in data.features if 'text' in d]\n",
    "            feature=feature[0]\n",
    "            text= [l[feature] for l in list(data_head)]\n",
    "        \n",
    "    else:\n",
    "        feature = next(iter(data.features))\n",
    "        text= data[feature][:kwargs['d_size']]\n",
    "    \n",
    "    encodings = tok('\\n\\n'.join(text), return_tensors='pt')\n",
    "    try:\n",
    "        max_length = model.config.max_position_embeddings\n",
    "    except AttributeError as error:\n",
    "        max_length = model.config.n_positions\n",
    "    except:\n",
    "        max_length=512\n",
    "        \n",
    "    #From https://huggingface.co/transformers/perplexity.html\n",
    "    stride = 512\n",
    "    lls = []\n",
    "    for i in range(0, encodings.input_ids.size(1), stride):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i    # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:,begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:,:-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "        lls.append(log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
    "    print(\"The perplexity of the \" + str(kwargs['m_name']) + \" model with the \" + str(kwargs['d_name']) + \" dataset is \" + str(ppl.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88ff559e-6ac8-48ad-befa-b451e69a3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combo_pp(models, datasets):\n",
    "    combos= list(itertools.product(models, datasets))\n",
    "    print(\"There are \" + str(len(list(combos))) + \" combinations of models and datasets.\")\n",
    "    for m, d in combos:\n",
    "        print('Analyzing the ' + m + ' model and the ' + d[0] + ' dataset.')\n",
    "        perplex_model_data(m_name= m, d_name= d[0], d_option= d[1], d_split= d[2], d_streaming= d[3], d_size= d[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf508891-2d61-4ab6-8d56-ffc270996855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 combinations of models and datasets.\n",
      "Analyzing the bert-base-uncased model and the oscar dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased model loaded!\n",
      "oscar dataset loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1918 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the bert-base-uncased model with the oscar dataset is 1.7199418544769287\n",
      "Analyzing the bert-base-uncased model and the imdb dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/sasha/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1342 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb dataset loaded!\n",
      "The perplexity of the bert-base-uncased model with the imdb dataset is 1.2695562839508057\n",
      "Analyzing the bert-base-uncased model and the poem_sentiment dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration plain_text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poem_sentiment dataset loaded!\n",
      "The perplexity of the bert-base-uncased model with the poem_sentiment dataset is 4.009187698364258\n",
      "Analyzing the bert-base-uncased model and the c4 dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased model loaded!\n",
      "c4 dataset loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the bert-base-uncased model with the c4 dataset is 1.4461493492126465\n",
      "Analyzing the t5-small model and the oscar dataset.\n",
      "oscar dataset loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2075 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the t5-small model with the oscar dataset is 541160.0625\n",
      "Analyzing the t5-small model and the imdb dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/sasha/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1511 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb dataset loaded!\n",
      "The perplexity of the t5-small model with the imdb dataset is 463074.34375\n",
      "Analyzing the t5-small model and the poem_sentiment dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration plain_text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poem_sentiment dataset loaded!\n",
      "The perplexity of the t5-small model with the poem_sentiment dataset is 1.4519227743148804\n",
      "Analyzing the t5-small model and the c4 dataset.\n",
      "c4 dataset loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the t5-small model with the c4 dataset is 186506.234375\n"
     ]
    }
   ],
   "source": [
    "get_combo_pp(mods,datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47822e1-c233-4560-b7db-a915efd46a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "base_url = \"https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/\"\n",
    "data_files = {\"train\": base_url + \"wikipedia-train.parquet\"}\n",
    "wiki = load_dataset(\"parquet\", data_files=data_files, split=\"train\", streaming=True)\n",
    "print(next(iter(wiki)))\n",
    "# {'title': 'Yangliuqing', 'text': 'Yangliuqing () is a market town in Xiqing District...'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f57922-e637-4aa8-8076-3b85d017a210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a002a9a-ac0f-4ab0-94ea-42b86b85ca59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2b0df-1465-499f-b3ca-14aa4e52dad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11c593-faca-4710-bd5c-6b41530bf2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datametrics",
   "language": "python",
   "name": "datametrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
