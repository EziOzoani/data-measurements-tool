{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690ce404-6783-4f54-9d1a-7f79f1fdab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb9cecf-8ff6-4a96-97b7-91ed5f90dc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422447d9d9ff4d3aa8b9b561dd0b4d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy is the proportion of correct predictions among the total number of cases processed. \\nIt can be computed with: \\nAccuracy = (TP + TN) / (TP + TN + FP + FN) TP: True positive TN: True negative FP: False positive FN: False negative\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = load_metric(\"accuracy\")\n",
    "'''Accuracy is the proportion of correct predictions among the total number of cases processed. \n",
    "It can be computed with: \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN) TP: True positive TN: True negative FP: False positive FN: False negative\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e48246-fb9f-47db-aa23-54fcd8af9407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nneeds pip install bert_score first \\nBERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences \\nby cosine similarity. It has been shown to correlate with human judgment on sentence-level and system-level evaluation. \\nMoreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different language generation tasks. \\nSee the `README.md` file at [https://github.com/Tiiiger/bert_score](https://github.com/Tiiiger/bert_score) for more information.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertscore = load_metric(\"bertscore\")\n",
    "'''\n",
    "Needs 'pip install bert_score' first!\n",
    "\n",
    "BERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences \n",
    "by cosine similarity. It has been shown to correlate with human judgment on sentence-level and system-level evaluation. \n",
    "Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different language generation tasks. \n",
    "See the `README.md` file at [https://github.com/Tiiiger/bert_score](https://github.com/Tiiiger/bert_score) for more information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf58ec49-1f23-4a62-a9e5-f2c53125f955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bcddb431ee451a9565f85b85c2a2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05623d3efa4d4f70805466081488cf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\nBLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated \\nfrom one natural language to another. Scores are calculated for individual translated segments—generally sentences—by comparing \\nthem with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate \\nof the translation's overall quality. Intelligibility or grammatical correctness are not taken into account[citation needed]. \\nBLEU's output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, \\nwith values closer to 1 representing more similar texts. Few human translations will attain a score of 1, since this would indicate \\nthat the candidate is identical to one of the reference translations. For this reason, it is not necessary to attain a score of 1.\\nBecause there are more opportunities to match, adding additional reference translations will increase the BLEU score.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = load_metric(\"bleu\")\n",
    "'''\n",
    "BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated \n",
    "from one natural language to another. Scores are calculated for individual translated segments—generally sentences—by comparing \n",
    "them with a set of good quality reference translations. Those scores are then averaged over the whole corpus to reach an estimate \n",
    "of the translation's overall quality. Intelligibility or grammatical correctness are not taken into account[citation needed]. \n",
    "BLEU's output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, \n",
    "with values closer to 1 representing more similar texts. Few human translations will attain a score of 1, since this would indicate \n",
    "that the candidate is identical to one of the reference translations. For this reason, it is not necessary to attain a score of 1.\n",
    "Because there are more opportunities to match, adding additional reference translations will increase the BLEU score.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc17d8f-0fc1-41b4-913d-d4b24b92d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleurt= load_metric(\"bleurt\")\n",
    "'''\n",
    "Needs 'pip install git+https://github.com/google-research/bleurt.git' \n",
    "BLEURT a learnt evaluation metric for Natural Language Generation. It is built using multiple phases of transfer learning \n",
    "starting from a pretrained BERT model (Devlin et al. 2018) and then employing another pre-training phrase using synthetic data. \n",
    "Finally it is trained on WMT human annotations. You may run BLEURT out-of-the-box or fine-tune it for your specific application \n",
    "(the latter is expected to perform better). See the [README.md] file at https://github.com/google-research/bleurt for more information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe92d32-3c04-4b39-9f63-9be17b4e41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet = load_metric(\"comet\")\n",
    "\n",
    "'''\n",
    "Needs 'pip install unbabel-comet' \n",
    "Crosslingual Optimized Metric for Evaluation of Translation (COMET) is an open-source framework used to train Machine Translation\n",
    "metrics that achieve high levels of correlation with different types of human judgments (HTER, DA's or MQM). \n",
    "With the release of the framework the authors also released fully trained models that were used to compete in the WMT20 Metrics \n",
    "Shared Task achieving SOTA in that years competition. See the [README.md] file at https://unbabel.github.io/COMET/html/models.html \n",
    "for more information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937b2eb2-36cc-4179-a6c8-b5e36f2954b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6f37fce3e647338740de3b4aa7370e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe F1 score is the harmonic mean of the precision and recall. \\nIt can be computed with: F1 = 2 * (precision * recall) / (precision + recall)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = load_metric(\"f1\")\n",
    "'''\n",
    "The F1 score is the harmonic mean of the precision and recall. \n",
    "It can be computed with: F1 = 2 * (precision * recall) / (precision + recall)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fcab4f-0efb-42f6-ad03-16a20491a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gleu= load_metric(\"gleu\")\n",
    "'''\n",
    "(under construction)\n",
    "The GLEU metric is a variant of BLEU proposed for evaluating grammatical error corrections using n-gram overlap \n",
    "with a set of reference sentences, as opposed to precision/recall of specific annotated errors (Napoles et al., 2015). \n",
    "GLEU hews more closely to human judgments than the rankings produced by metrics such as MaxMatch and I-measure. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b77adea-8c35-409f-8b3c-356207aaf51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e090ffe0ff4cea99d936914d5236be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications.\\nIt takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even \\nif the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. \\nA coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. \\nThe statistic is also known as the phi coefficient. [source: Wikipedia]\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcc = load_metric(\"matthews_correlation\")\n",
    "'''\n",
    "The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications.\n",
    "It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even \n",
    "if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. \n",
    "A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. \n",
    "The statistic is also known as the phi coefficient. [source: Wikipedia]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96e22247-028c-4f59-8d8b-6f49e524327a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2f31529ed14ca3bf9ec1a9fd1e47b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sasha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMETEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching \\nbetween the machine-produced translation and human-produced reference translations. Unigrams can be matched based on their \\nsurface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching \\nstrategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score \\nfor this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to \\ndirectly capture how well-ordered the matched words in the machine translation are in relation to the reference. \\nMETEOR gets an R correlation value of 0.347 with human evaluation on the Arabic data and 0.331 on the Chinese data. \\nThis is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor = load_metric(\"meteor\")\n",
    "'''\n",
    "METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching \n",
    "between the machine-produced translation and human-produced reference translations. Unigrams can be matched based on their \n",
    "surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching \n",
    "strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score \n",
    "for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to \n",
    "directly capture how well-ordered the matched words in the machine translation are in relation to the reference. \n",
    "METEOR gets an R correlation value of 0.347 with human evaluation on the Arabic data and 0.331 on the Chinese data. \n",
    "This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6d2967d-4663-4d62-8f1d-07b025173cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac5568e64bd470bac4d702c2ae7d05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nPearson correlation coefficient and p-value for testing non-correlation. The Pearson correlation coefficient measures \\nthe linear relationship between two datasets. The calculation of the p-value relies on the assumption that each dataset \\nis normally distributed. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. \\nCorrelations of -1 or +1 imply an exact linear relationship. Positive correlations imply that as x increases, so does y. \\nNegative correlations imply that as x increases, y decreases. The p-value roughly indicates the probability of an uncorrelated system \\nproducing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr = load_metric(\"pearsonr\")\n",
    "\n",
    "'''\n",
    "Pearson correlation coefficient and p-value for testing non-correlation. The Pearson correlation coefficient measures \n",
    "the linear relationship between two datasets. The calculation of the p-value relies on the assumption that each dataset \n",
    "is normally distributed. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. \n",
    "Correlations of -1 or +1 imply an exact linear relationship. Positive correlations imply that as x increases, so does y. \n",
    "Negative correlations imply that as x increases, y decreases. The p-value roughly indicates the probability of an uncorrelated system \n",
    "producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "738212b0-a7a5-425c-ae6a-155509243d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2438eaaa713b4ace84c31583f834a98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nPrecision is the fraction of the true examples among the predicted examples. It can be computed with: \\nPrecision = TP / (TP + FP) TP: True positive FP: False positive\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = load_metric(\"precision\")\n",
    "\n",
    "'''\n",
    "Precision is the fraction of the true examples among the predicted examples. It can be computed with: \n",
    "Precision = TP / (TP + FP) TP: True positive FP: False positive\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06ae41de-60b1-461f-b347-3b1b497f818d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd6eadc36a54606b7ea9986b41e6c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nRecall is the fraction of the total amount of relevant examples that were actually retrieved. It can be computed with: \\nPrecision = TP / (TP + FN) TP: True positive FN: False negative\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = load_metric(\"recall\")\n",
    "'''\n",
    "Recall is the fraction of the total amount of relevant examples that were actually retrieved. It can be computed with: \n",
    "Precision = TP / (TP + FN) TP: True positive FN: False negative\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94f43071-784d-48a4-99a5-8a1b61200b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNeeds \\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating \\nautomatic summarization and machine translation software in natural language processing. The metrics compare an automatically \\nproduced summary or translation against a reference or a set of references (human-produced) summary or translation. \\nNote that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters. \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "'''\n",
    "Needs 'pip install rouge_score'\n",
    "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating \n",
    "automatic summarization and machine translation software in natural language processing. The metrics compare an automatically \n",
    "produced summary or translation against a reference or a set of references (human-produced) summary or translation. \n",
    "Note that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc54efca-2648-417a-b312-9812774c7122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 62 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5230d18e-5b27-4da5-9638-8c37af4af5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nNeeds 'pip install sacrebleu'\\nSacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. \\nInspired by Rico Sennrich's `multi-bleu-detok.perl`, it produces the official WMT scores but works with plain text. \\nIt also knows all the standard test sets and handles downloading, processing, and tokenization for you. \\nSee the [README.md] file at https://github.com/mjpost/sacreBLEU for more information.\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sacrebleu = load_metric(\"sacrebleu\")\n",
    "''''\n",
    "Needs 'pip install sacrebleu'\n",
    "SacreBLEU provides hassle-free computation of shareable, comparable, and reproducible BLEU scores. \n",
    "Inspired by Rico Sennrich's `multi-bleu-detok.perl`, it produces the official WMT scores but works with plain text. \n",
    "It also knows all the standard test sets and handles downloading, processing, and tokenization for you. \n",
    "See the [README.md] file at https://github.com/mjpost/sacreBLEU for more information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc55a05f-25fe-45ec-91b8-8c8ca1317b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af5015fbb724243ac26769f494da1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nSARI is a metric used for evaluating automatic text simplification systems. The metric compares the predicted simplified \\nsentences against the reference and the source sentences. It explicitly measures the goodness of words that are added, \\ndeleted and kept by the system. Sari = (F1_add + F1_keep + P_del) / 3 where F1_add: n-gram F1 score for add operation \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sari = load_metric(\"sari\")\n",
    "'''\n",
    "SARI is a metric used for evaluating automatic text simplification systems. The metric compares the predicted simplified \n",
    "sentences against the reference and the source sentences. It explicitly measures the goodness of words that are added, \n",
    "deleted and kept by the system. Sari = (F1_add + F1_keep + P_del) / 3 where F1_add: n-gram F1 score for add operation \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bef549b-0ae1-464e-b703-cba5c1c629c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNeeds 'pip install seqeval' \\nseqeval is a Python framework for sequence labeling evaluation. seqeval can evaluate the performance of chunking tasks \\nsuch as named-entity recognition, part-of-speech tagging, semantic role labeling and so on. This is well-tested by using \\nthe Perl script conlleval, which can be used for measuring the performance of a system that has processed the CoNLL-2000 \\nshared task data. seqeval supports following formats: IOB1 IOB2 IOE1 IOE2 IOBES \\nSee the [README.md] file at https://github.com/chakki-works/seqeval for more information.\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqeval = load_metric(\"seqeval\")\n",
    "'''\n",
    "Needs 'pip install seqeval' \n",
    "seqeval is a Python framework for sequence labeling evaluation. seqeval can evaluate the performance of chunking tasks \n",
    "such as named-entity recognition, part-of-speech tagging, semantic role labeling and so on. This is well-tested by using \n",
    "the Perl script conlleval, which can be used for measuring the performance of a system that has processed the CoNLL-2000 \n",
    "shared task data. seqeval supports following formats: IOB1 IOB2 IOE1 IOE2 IOBES \n",
    "See the [README.md] file at https://github.com/chakki-works/seqeval for more information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ce1bfe0-2749-494b-b953-80f0ed350521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90ac660958f4a8c890f8966c615763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nCalculate a Spearman correlation coefficient with associated p-value. The Spearman rank-order correlation coefficient \\nis a nonparametric measure of the monotonicity of the relationship between two datasets. Unlike the Pearson correlation, \\nthe Spearman correlation does not assume that both datasets are normally distributed. Like other correlation coefficients, \\nthis one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. \\nPositive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases. \\nThe p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation \\nat least as extreme as the one computed from these datasets. The p-values are not entirely reliable but are probably \\nreasonable for datasets larger than 500 or so.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr = load_metric(\"spearmanr\")\n",
    "'''\n",
    "Calculate a Spearman correlation coefficient with associated p-value. The Spearman rank-order correlation coefficient \n",
    "is a nonparametric measure of the monotonicity of the relationship between two datasets. Unlike the Pearson correlation, \n",
    "the Spearman correlation does not assume that both datasets are normally distributed. Like other correlation coefficients, \n",
    "this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. \n",
    "Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases. \n",
    "The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation \n",
    "at least as extreme as the one computed from these datasets. The p-values are not entirely reliable but are probably \n",
    "reasonable for datasets larger than 500 or so.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "340b831b-d15a-4885-bc01-8d4fcef84f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761c55809fbe44a2b3b4390f089b3e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97679b9de3ea4c4994daa3338fbce114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nThis metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD). \\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by \\ncrowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, \\nfrom the corresponding reading passage, or the question might be unanswerable.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad= load_metric(\"squad\")\n",
    "'''\n",
    "This metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD). \n",
    "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by \n",
    "crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, \n",
    "from the corresponding reading passage, or the question might be unanswerable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75bb5731-8701-4ff6-96f3-815ea4b49159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738b58c750a345c199562fb46287294f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776136c2caeb49b687e1f0d22f071ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nThis metric wrap the official scoring script for version 2 of the Stanford Question Answering Dataset (SQuAD). \\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by \\ncrowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,\\nfrom the corresponding reading passage, or the question might be unanswerable. \\nSQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially \\nby crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions \\nwhen possible, but also determine when no answer is supported by the paragraph and abstain from answering.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_v2 = load_metric(\"squad_v2\")\n",
    "'''\n",
    "This metric wrap the official scoring script for version 2 of the Stanford Question Answering Dataset (SQuAD). \n",
    "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by \n",
    "crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,\n",
    "from the corresponding reading passage, or the question might be unanswerable. \n",
    "SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially \n",
    "by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions \n",
    "when possible, but also determine when no answer is supported by the paragraph and abstain from answering.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e75115b2-a10c-4895-82e5-22961edb89d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou should supply a configuration name selected in \\n[\"boolq\", \"cb\", \"copa\", \"multirc\", \"record\", \"rte\", \"wic\", \"wsc\", \"wsc.fixed\", \"axb\", \"axg\"]\\'\\nSuperGLUE (https://super.gluebenchmark.com/) is a new benchmark styled after GLUE with a new set of more difficult \\nlanguage understanding tasks, improved resources, and a new public leaderboard.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superglue = load_metric(\"super_glue\", \"axb\")\n",
    "'''\n",
    "You should supply a configuration name selected in \n",
    "[\"boolq\", \"cb\", \"copa\", \"multirc\", \"record\", \"rte\", \"wic\", \"wsc\", \"wsc.fixed\", \"axb\", \"axg\"]'\n",
    "SuperGLUE (https://super.gluebenchmark.com/) is a new benchmark styled after GLUE with a new set of more difficult \n",
    "language understanding tasks, improved resources, and a new public leaderboard.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afed5af8-ba32-4759-8644-d8148ec71d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a06d236e39416fa2459f15e5d7b9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nWIKI_SPLIT is the combination of three metrics SARI, EXACT and SACREBLEU.\\nIt can be used to evaluate the quality of machine-generated texts.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_split = load_metric(\"wiki_split\")\n",
    "'''\n",
    "WIKI_SPLIT is the combination of three metrics SARI, EXACT and SACREBLEU.\n",
    "It can be used to evaluate the quality of machine-generated texts.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16c68374-8b24-4b9c-a787-7b6efcbbf7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a52c35f1cc4c39ba0b33323ccec75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nXNLI is a subset of a few thousand examples from MNLI which has been translated into a 14 different languages \\n(some low-ish resource). As with MNLI, the goal is to predict textual entailment (does sentence A imply/contradict/neither sentence B)\\nand is a classification task (given two sentences, predict one of three labels).\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli = load_metric(\"xnli\")\n",
    "'''\n",
    "XNLI is a subset of a few thousand examples from MNLI which has been translated into a 14 different languages \n",
    "(some low-ish resource). As with MNLI, the goal is to predict textual entailment (does sentence A imply/contradict/neither sentence B)\n",
    "and is a classification task (given two sentences, predict one of three labels).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60eda7-3012-4044-9ccc-ff557ae158c3",
   "metadata": {},
   "source": [
    "Not included:\n",
    "- cer\n",
    "- coval\n",
    "- cuad\n",
    "- indic_glue\n",
    "- wer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datametrics",
   "language": "python",
   "name": "datametrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
