{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf885d68-6d8d-480a-8bc4-e6dc909ab80b",
   "metadata": {},
   "source": [
    "# Zipf it up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d398b2d-f9de-47ed-80ef-b89afc9af09c",
   "metadata": {},
   "source": [
    "## Zipf's law: Given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.\n",
    "### This follows a basic power law (but with additional parameters):\n",
    "$$ {\\displaystyle{\\displaystyle p(x)  \\propto x^{\\alpha}}} $$\n",
    "### Zipf's law predicts that out of a population of $N$ elements, the normalized frequency of the element of rank $k$, $f(k;\\alpha,N)$, is:\n",
    "\n",
    "$${\\displaystyle f(k;\\alpha,N)={\\frac {1/k^{\\alpha}}{\\sum \\limits _{n=1}^{N}(1/n^{\\alpha})}}}$$\n",
    "\n",
    "#### A dataset can be tested to see whether Zipf's law applies by checking the \"goodness of fit\" of the dataset's observed rank frequency distribution to a hypothesized power law distribution, by using a Kolmogorovâ€“Smirnov test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d32bb1-930f-43c5-a78b-205d8bbf8064",
   "metadata": {},
   "source": [
    "#### This code does all that stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00eca3a3-7acf-43e3-8e99-8b5cb9493609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grepping the installation babbling basically just \n",
    "# suppresses the warnings by providing a command that will find them but won't print them.\n",
    "!pip install powerlaw | grep -v 'Requirement already satisfied' \n",
    "import powerlaw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import hmean, norm, ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96d00b5-0953-45ea-90c9-854d56791066",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= load_dataset(\"oscar\", \"unshuffled_deduplicated_en\", split = \"train\", streaming= True)\n",
    "#data = load_dataset(\"c4\", \"en\", split= \"train\", streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10482904-64fd-4354-a00c-e5c5f596543d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Just taking the first 1000 instances.\n"
     ]
    }
   ],
   "source": [
    "# For streaming data\n",
    "print('Note: Just taking the first 1000 instances.')\n",
    "data_head = data.take(1000)\n",
    "df = pd.DataFrame(data_head)\n",
    "# If not streaming, use:\n",
    "#df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b4dd7-25d1-44ee-8506-8a67d350b639",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Look at the top rows as a sanity-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc73f02-cfc3-4cf6-8f77-8c6bd0cdb6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mtendere Village was inspired by the vision of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lily James cannot fight the music. In the titl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"I'd love to help kickstart continued developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"We view 11-11 as a top-tier, strategic partne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you looking for Number the Stars (Essentia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0  Mtendere Village was inspired by the vision of...\n",
       "1   1  Lily James cannot fight the music. In the titl...\n",
       "2   2  \"I'd love to help kickstart continued developm...\n",
       "3   3  \"We view 11-11 as a top-tier, strategic partne...\n",
       "4   4  Are you looking for Number the Stars (Essentia..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899197c1-8f35-4518-ac0b-2884b2e228cc",
   "metadata": {},
   "source": [
    "### Get more basic sanity-check information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3aef2d-564d-43c8-be3c-a3721a35483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      1000 non-null   int64 \n",
      " 1   text    1000 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8acd2c-98f4-4e44-b75b-37299eca868e",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83f149c-eba8-45c8-96be-0e2aa8bb2c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)].head()\n",
    "np.sum(df.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf363d-32b0-4f9e-957b-673d20d5f2fa",
   "metadata": {},
   "source": [
    "### Count vocab size + frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a1c847-388c-4862-a66f-8b480f800bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vocab_frequencies(df, cutoff=3):\n",
    "    \"\"\"\n",
    "    Based on an input pandas DataFrame with a 'text' column, \n",
    "    this function will count the occurrences of all words\n",
    "    with a frequency higher than 'cutoff' and will return another DataFrame\n",
    "    with the rows corresponding to the different vocabulary words\n",
    "    and the column to the total count of that word.\n",
    "    \"\"\"\n",
    "    # Move this up as a constant in larger code.\n",
    "    batch_size = 10\n",
    "    cvec = CountVectorizer(token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "    # Needed to modify the minimum token length: \n",
    "    # https://stackoverflow.com/questions/33260505/countvectorizer-ignoring-i\n",
    "    cvec.fit(df.text)\n",
    "    document_matrix = cvec.transform(df.text)\n",
    "    batches = np.linspace(0, df.shape[0], batch_size).astype(int)\n",
    "    i = 0\n",
    "    tf = []\n",
    "    while i < len(batches) - 1:\n",
    "        batch_result = np.sum(document_matrix[batches[i]:batches[i+1]].toarray(), axis=0)\n",
    "        tf.append(batch_result)\n",
    "        i += 1\n",
    "    term_freq_df = pd.DataFrame([np.sum(tf, axis=0)], columns=cvec.get_feature_names()).transpose()\n",
    "    term_freq_df.columns = ['total']\n",
    "    term_freq_df = term_freq_df[term_freq_df['total'] > cutoff]\n",
    "    sorted_term_freq_df = pd.DataFrame(term_freq_df.sort_values(by='total')['total'])\n",
    "    return sorted_term_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb931a64-d81b-4505-8f5b-e7f9e09df8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info on the observed frequencies:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 16257 entries, jaws to the\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   total   16257 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 254.0+ KB\n",
      "None\n",
      "------------------------------------\n",
      "Vocab size (types):\t16257\n",
      "Vocab size (tokens):\t1005258\n",
      "Observations look like this:\n",
      "     total\n",
      "a    22043\n",
      "of   24251\n",
      "to   28256\n",
      "and  29384\n",
      "the  50142\n"
     ]
    }
   ],
   "source": [
    "term_df = count_vocab_frequencies(df)\n",
    "print(\"Info on the observed frequencies:\")\n",
    "print(term_df.info())\n",
    "print(\"------------------------------------\")\n",
    "print(\"Vocab size (types):\\t%s\" % len(term_df))\n",
    "print(\"Vocab size (tokens):\\t%s\" % sum(term_df['total']))\n",
    "print(\"Observations look like this:\")\n",
    "print(term_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e58da4-0d4a-4424-b327-7c32218a0c9e",
   "metadata": {},
   "source": [
    "### Fit the observed totals to a Zipfian powerlaw.\n",
    "#### Because the ranks are integers, we treat it as a discrete distribution (actually should be 'ordinal' distribution). Our reasoning is that it doesn't make sense to mandate that the distances between the ranks are equal; the values are 'ordinal' actually.\n",
    "#### We fit this distribution to a power law by minimizing the Kolmogorovâ€“Smirnov (KS) distance.\n",
    "#### Our reasoning in using a KS objective rather than Maximun Likelihood Estimation (MLE) is that a fundamental assumption of MLE is that individual data points are independent, but that oughtn't be so with the use of words in language (we think)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbdb91a6-6985-4049-9d40-07664aaef5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating best minimal value for power law fit\n",
      "------------------------------------\n",
      "Optimal Alpha:\t\t2.0362\n",
      "Optimal Frequency cut-off:\t57.0\n",
      "Distance:\t\t0.0230\n",
      "Checking the goodness of fit of our observed distribution\n",
      " to the hypothesized power law distribution\n",
      " using a Kolmogorovâ€“Smirnov (KS) test.\n",
      "KstestResult(statistic=0.5145956383462063, pvalue=3.589876174103779e-10)\n",
      "\n",
      "The KS test p-value is: 0.0000\n",
      "\n",
      "Your data fits a powerlaw with a minimum KS distance of 0.0230\n",
      "\n",
      "Woohoo!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Uses the powerlaw package to fit the observed frequencies to a zipfian distribution\n",
    "\n",
    "observed_counts = np.flip(term_df['total'].values)\n",
    "# Turn these into an empirical probability distribution by normalizing by the total sum.\n",
    "# Note -- doesn't seem to matter actually; can remove.\n",
    "norm = float(sum(observed_counts))\n",
    "observed_probabilities = observed_counts/norm\n",
    "#observed_log_probabilities = np.log(observed_counts) - np.log(norm)\n",
    "# 'fit_method' is MLE by default; doesn't seem to change the results in my initial pokings.\n",
    "# Also tried discrete_approximation=\"xmax\"\n",
    "# Note another method for determining alpha \n",
    "# might be defined by (Newman, 2005 for details): alpha = 1 + n * sum(ln( xi / xmin )) ^ -1\n",
    "fit = powerlaw.Fit(observed_counts, fit_method=\"KS\", discrete=True)\n",
    "# bins_edges = The edges of the bins of the probability density function.\n",
    "# The portion of the data that is within the bin. Length 1 less than bin_edges, as it corresponds to the spaces between them.\n",
    "# .pdf() returns the probability density function (normalized histogram) of the theoretical distribution\n",
    "pdf_bin_edges, observed_pdf = fit.pdf(original_data=True)\n",
    "# The likelihoods of the observed data from the theoretical distribution.\n",
    "predicted_likelihoods = fit.power_law.likelihoods\n",
    "# The logarithm of the likelihoods of the observed data from the theoretical distribution.\n",
    "predicted_log_likelihoods = fit.power_law.loglikelihoods\n",
    "# The estimated values\n",
    "#predicted_probabilities = np.flip(bin_edges/norm)\n",
    "# power_law.pdf() returns the probability density function (normalized histogram) of the data.\n",
    "predicted_pdf = fit.power_law.pdf()\n",
    "alpha = fit.power_law.alpha\n",
    "xmin = fit.power_law.xmin\n",
    "distance = fit.power_law.KS()\n",
    "\n",
    "\n",
    "# We know the optimal xmin, but now let's set the prediction xmin to a lower value\n",
    "# so we can graph out more beyond the cutoff.\n",
    "# Note: Bring this up as a parameter that can be set.\n",
    "# fit.xmin = .001\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Optimal Alpha:\\t\\t%.4f\" % alpha)\n",
    "print(\"Optimal Frequency cut-off:\\t%s\" % xmin)\n",
    "print(\"Distance:\\t\\t%.4f\" % distance)\n",
    "# Significance testing\n",
    "# Note: We may want to use bootstrapping (instead of the standard KS test p-value tables) to determine statistical significance\n",
    "# See: https://stats.stackexchange.com/questions/264431/how-to-determine-if-zipfs-law-can-be-applied Answer #4\n",
    "print(\"Checking the goodness of fit of our observed distribution\")\n",
    "print(\" to the hypothesized power law distribution\")\n",
    "print(\" using a Kolmogorovâ€“Smirnov (KS) test.\")\n",
    "ks_test = ks_2samp(observed_pdf, predicted_pdf)\n",
    "# print(\"KS test:\", end='\\t\\t')\n",
    "print(ks_test)\n",
    "print(\"\\nThe KS test p-value is: %.4f\" % ks_test.pvalue)\n",
    "if ks_test.pvalue < .01:\n",
    "    print(\"\\nYour data fits a powerlaw with a minimum KS distance of %.4f\" % distance)\n",
    "    print(\"\\nWoohoo!\")\n",
    "else:\n",
    "    print(\"\\nYour data does not fits a powerlaw. =\\(\")\n",
    "    print(\"\\nDO BETTER.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ea3e8-db27-4538-b0c4-e8dc663687b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A straightforward application of Zipf's law with that alpha (?) isn't returning what I expect...is it? =/\")\n",
    "rank_1_num = 1/2**2.04\n",
    "rank_1_denom = sum(1/observed_count**2.04 for observed_count in observed_counts)\n",
    "#rank_1_denom = len(observed_counts)\n",
    "print(rank_1_num/rank_1_denom)\n",
    "print(rank_1_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4b904-a5c4-4ac0-9a13-7bae8a005bba",
   "metadata": {},
   "source": [
    "### WARNING WARNING MAJOR HACKY LAND!!  \n",
    "#### Taking the original ranks, corresponding to words, and calculating the corresponding probability from the fitted model at that rank.\n",
    "#### Doing this basically in the slowest way possible because I was getting frustrated with fancier tools not working so I just hacked through inelegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed27ede-13d8-4eb7-9322-24307a913104",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bin_edges' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-24b3603fe3de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# while the predicted count is higher than the observed count,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# set its rank to the observed rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mobserved_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mbin_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bin_edges' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "predicted_per_rank = defaultdict(list)\n",
    "j = 0\n",
    "# For each rank in the observed_counts\n",
    "for i in range(len(observed_counts)):\n",
    "    observed_count = observed_counts[i]\n",
    "    rank = i+1\n",
    "    # while the predicted count is higher than the observed count,\n",
    "    # set its rank to the observed rank\n",
    "    if j < len(bin_edges):\n",
    "        while np.flip(bin_edges)[j] >= observed_count:\n",
    "            bin_rank = rank\n",
    "            j +=1\n",
    "            predicted_per_rank[i] += [np.flip(bin_edges)[j-1]]\n",
    "            if (j>=len(bin_edges)):\n",
    "                break\n",
    "\n",
    "predicted_x_axis = []\n",
    "predicted_y_axis = []\n",
    "for i, j in sorted(predicted_per_rank.items()):\n",
    "    predicted_x_axis += [i]\n",
    "    predicted_y_axis += [sum(j)/len(j)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50131b8-eda6-4738-93d1-29e656e554dc",
   "metadata": {},
   "source": [
    "#### Create data structure for mapping the predictions to the original zipf histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b9362-3f02-429a-a0e7-e29dafc5feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_set = np.flip(term_df['total']/norm)[:len(predicted_y_axis)]\n",
    "predicted_set = np.array(predicted_y_axis)/norm\n",
    "zipf_data = pd.DataFrame(original_set)\n",
    "zipf_data['predicted'] = predicted_set\n",
    "print(zipf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704368f-a845-4dfa-ba4b-cb5a40f55949",
   "metadata": {},
   "source": [
    "### Graph out the observed and fitted pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf71ed-fc6d-421e-9a0d-ff560a4665b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph it out.\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "# The pdf of the observed data. \n",
    "# The continuous line is superfluous and confusing I think (?) Hacky removing.\n",
    "fit.plot_pdf(color='r', linewidth=0, linestyle=':', marker='o')\n",
    "# The pdf of the best fit powerlaw\n",
    "fit.power_law.plot_pdf(color='b', linestyle='--', linewidth=2)\n",
    "fig.suptitle('Log-log plot of word frequency (y-axis) vs. rank (x-axis) \\nObserved = red dots\\n Power law = blue lines', fontsize=20)\n",
    "plt.ylabel('Log frequency', fontsize=18)\n",
    "plt.xlabel('Log rank (binned frequencies).\\nGaps signify there weren\\'t observed words in the fitted frequency bin.', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63181289-1b2a-452e-ac60-80d0c359323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zipf(observed, predicted_x, predicted_y, alpha=1, multiplier=1):\n",
    "    \"\"\"\n",
    "    This function plots the correspondence of the distribution of terms\n",
    "    generated by count_vocab_frequencies with the projected Zipf's law\n",
    "    distribution.\n",
    "    \"\"\"\n",
    "    num_tokens = len(observed)\n",
    "    max_rank = 50\n",
    "    y_bins = np.arange(max_rank)\n",
    "    smaller_predicted_x = predicted_x[predicted_x <= max_rank]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.bar(y_bins, observed[:max_rank]*multiplier, align='center', alpha=0.5)\n",
    "    plt.plot(smaller_predicted_x, predicted_y[:len(smaller_predicted_x)]*multiplier, color='r', linestyle='--',linewidth=2,alpha=0.5)\n",
    "    plt.ylabel('Normalized Frequency')\n",
    "    plt.xlabel('Bin')\n",
    "    plt.title(\"Top %s ranks in the dataset, with predictions from a fitted power law in dotted red\" % max_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217960f-377e-4fec-81e9-ca76bc9096a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean this up so you're not making these calculations here.\n",
    "plot_zipf(observed_probabilities, np.array(predicted_x_axis), predicted_set, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb5132-2cba-40f1-9aa7-34e37b31a337",
   "metadata": {},
   "source": [
    "#### Check fit to other distributions.\n",
    "#### This is comparing the log likelihood ratio of the power law distribution to alternative distributions: a truncated power law, an exponential distribution or a lognormal distribution.\n",
    "#### NB: There's an issue here with handling correlations.\n",
    "#### I'm not sure this is the best way to do this, esp bc it throws errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b0f71-9047-4bfe-89ec-57fa6cc2d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking log likelihood ratio to see if the data is better explained\")\n",
    "print(\"by other well-behaved distributions...\")\n",
    "# The first value returned from distribution_compare is the log likelihood ratio\n",
    "better_distro = False\n",
    "trunc = fit.distribution_compare('power_law', 'truncated_power_law')\n",
    "if trunc[0] < 0:\n",
    "    print(\"Seems a truncated power law is a better fit.\")\n",
    "    better_distro = True\n",
    "\n",
    "lognormal = fit.distribution_compare('power_law', 'lognormal')\n",
    "if lognormal[0] < 0:\n",
    "    print(\"Seems a lognormal distribution is a better fit.\")\n",
    "    print(\"But don't panic -- that happens sometimes with language.\")\n",
    "    better_distro = True\n",
    "    \n",
    "exponential = fit.distribution_compare('power_law', 'exponential')\n",
    "if exponential[0] < 0:\n",
    "    print(\"Seems an exponential distribution is a better fit. Panic.\")\n",
    "    better_distro = True\n",
    "    \n",
    "if not better_distro:\n",
    "    print(\"\\nSeems your data is best fit by a power law. Celebrate!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
