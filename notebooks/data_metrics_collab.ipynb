{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "1. [Vocabulary Size](#Vocabulary-Size)\n",
    "2. [Instance Characteristics](#Instance-Characteristics)\n",
    "3. [Perplexity](#Perplexity-based-on-Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (0.1.96)\n",
      "Requirement already satisfied: ipywidgets in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (7.6.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipywidgets) (7.25.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipywidgets) (6.0.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipywidgets) (1.0.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipywidgets) (5.0.5)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: appnope in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.4.1)\n",
      "Requirement already satisfied: jupyter-client<7.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: pickleshare in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: pygments in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (2.9.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.19)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (49.6.0.post20210108)\n",
      "Requirement already satisfied: backcall in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (5.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from jupyter-client<7.0->ipykernel>=4.5.1->ipywidgets) (22.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from jupyter-client<7.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from jupyter-client<7.0->ipykernel>=4.5.1->ipywidgets) (4.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.0)\n",
      "Requirement already satisfied: argon2-cffi in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.11.0)\n",
      "Requirement already satisfied: nbconvert in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.10.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: bleach in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.3.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: testpath in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: async-generator in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\r\n",
      "Requirement already satisfied: nest-asyncio in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.1)\r\n",
      "Requirement already satisfied: packaging in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.0)\r\n",
      "Requirement already satisfied: webencodings in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/margaretmitchell/mambaforge/lib/python3.9/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\r\n"
     ]
    }
   ],
   "source": [
    "# Making sure the less common dependencies in this notebook are available for a user.\n",
    "import sys\n",
    "!{sys.executable} -m pip install sentencepiece\n",
    "!{sys.executable} -m pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/margaretmitchell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/margaretmitchell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used later in vocab statistics.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, RegexpTokenizer, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating metrics on different datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 'Preprocessing' step -- Preprocessing should be in its own module\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_basics(input_data, label_column, json_column=False, label_type='discrete'):\n",
    "    df = pd.DataFrame.from_dict(input_data)\n",
    "    print(\"* Peek at data:\")\n",
    "    print(df.head())\n",
    "    if json_column:\n",
    "        df = pd.json_normalize(df[json_column])\n",
    "        print(\"\\n* Peek at data:\")\n",
    "        print(df.head())\n",
    "    data_shape = df.shape\n",
    "    print(\"\\nNumber of rows: %s\" % data_shape[0])\n",
    "    print(\"\\nNumber of columns: %s\" % data_shape[1])\n",
    "    print(\"\\n* Label distribution:\")\n",
    "    if label_type == \"discrete\":\n",
    "        print(df[label_column].value_counts())\n",
    "    elif label_type == \"real\":\n",
    "        np_array = np.array(df[label_column])\n",
    "        print(\"Min:\", np_array.min())\n",
    "        print(\"Max:\", np_array.max())\n",
    "        print(\"Mean:\", np_array.mean())\n",
    "        print(\"Variance:\", np_array.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_count_vocab(input_data, lower=True, language=\"english\"):\n",
    "    # Counts the number of tokens, with or without lowercase normalization.\n",
    "    tokenized_text = tokenizer.tokenize(input_data)\n",
    "    language_stopwords = stopwords.words(language)\n",
    "    if lower:\n",
    "        vocab = FreqDist(word.lower() for word in tokenized_text)\n",
    "        # Are all the stopwords in lowercase?\n",
    "        filtered_vocab = FreqDist(word.lower() for word in tokenized_text if word.lower() not in language_stopwords)\n",
    "        lem_vocab = FreqDist(wnl.lemmatize(word.lower()) for word in tokenized_text if word.lower() not in language_stopwords)\n",
    "    else:\n",
    "        vocab = FreqDist(word for word in tokenized_text)\n",
    "        filtered_vocab = FreqDist(word for word in tokenized_text if word not in language_stopwords)\n",
    "        lem_vocab = FreqDist(wnl.lemmatize(word for word in tokenized_text if word not in language_stopwords))\n",
    "    print(\"There are \" + str(len(vocab)) + \" words including stop words\")\n",
    "    print(\"There are \" + str(len(filtered_vocab)) + \" words after removing stop words\")\n",
    "    print(\"There are \" + str(len(lem_vocab)) + \" words after removing stop words and lemmatizing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text_stats(text_list):\n",
    "    # Calculates sufficient statistics for text-based instances: average, mean, medium\n",
    "    total_lens = 0\n",
    "    alllengths=[]\n",
    "    for i, sent in enumerate(text_list):\n",
    "        lent=len(tokenizer.tokenize(sent))\n",
    "        alllengths.append(lent)\n",
    "        total_lens += lent\n",
    "    avg_sent_len = total_lens / i\n",
    "    print(\"The average sentence length is: \" + str(round(avg_sent_len,4)) + \" words.\")\n",
    "    print(\"The mean sentence length is: \" + str(statistics.mean(alllengths)) + \" words.\")\n",
    "    print(\"The mean sentence length is: \" + str(statistics.median(alllengths)) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Label Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD. Sasha had focused on imdb: most frequent words for each label, \n",
    "# and words only present in the top 10,000 most common positive/negative words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: glue-ax\n",
    "A manually-curated evaluation dataset for fine-grained analysis of system performance on a broad range of linguistic phenomena. This dataset evaluates sentence understanding through Natural Language Inference (NLI) problems. Use a model trained on MulitNLI to produce predictions for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/Users/margaretmitchell/.cache/huggingface/datasets/glue/ax/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"ax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Peek at data:\n",
      "                                                test\n",
      "0  {'premise': 'The cat sat on the mat.', 'hypoth...\n",
      "1  {'premise': 'The cat did not sit on the mat.',...\n",
      "2  {'premise': 'When you've got no snow, it's rea...\n",
      "3  {'premise': 'When you've got snow, it's really...\n",
      "4  {'premise': 'Out of the box, Ouya supports med...\n",
      "\n",
      "* Peek at data:\n",
      "                                             premise  \\\n",
      "0                            The cat sat on the mat.   \n",
      "1                    The cat did not sit on the mat.   \n",
      "2  When you've got no snow, it's really hard to l...   \n",
      "3  When you've got snow, it's really hard to lear...   \n",
      "4  Out of the box, Ouya supports media apps such ...   \n",
      "\n",
      "                                          hypothesis  label  idx  \n",
      "0                    The cat did not sit on the mat.     -1    0  \n",
      "1                            The cat sat on the mat.     -1    1  \n",
      "2  When you've got snow, it's really hard to lear...     -1    2  \n",
      "3  When you've got no snow, it's really hard to l...     -1    3  \n",
      "4  Out of the box, Ouya doesn't support media app...     -1    4  \n",
      "\n",
      "Number of rows: 1104\n",
      "\n",
      "Number of columns: 4\n",
      "\n",
      "* Label distribution:\n",
      "-1    1104\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_data_basics(dataset, label_column=\"label\", json_column=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset asset (/Users/margaretmitchell/.cache/huggingface/datasets/asset/ratings/1.0.0/62758c1bd7c109dfcf3d963fe61bc31625ce223c45bbe0df4ad72b9f5ce4f3ae)\n"
     ]
    }
   ],
   "source": [
    "asset = load_dataset(\"asset\", \"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Peek at data:\n",
      "                                                full\n",
      "0  {'original': 'Since 2000, the recipient of the...\n",
      "1  {'original': 'Since 2000, the recipient of the...\n",
      "2  {'original': 'Since 2000, the recipient of the...\n",
      "3  {'original': 'Since 2000, the recipient of the...\n",
      "4  {'original': 'Since 2000, the recipient of the...\n",
      "\n",
      "* Peek at data:\n",
      "                                            original  \\\n",
      "0  Since 2000, the recipient of the Kate Greenawa...   \n",
      "1  Since 2000, the recipient of the Kate Greenawa...   \n",
      "2  Since 2000, the recipient of the Kate Greenawa...   \n",
      "3  Since 2000, the recipient of the Kate Greenawa...   \n",
      "4  Since 2000, the recipient of the Kate Greenawa...   \n",
      "\n",
      "                                      simplification  original_sentence_id  \\\n",
      "0  Since 2000, the winner of the Kate Greenaway m...                     7   \n",
      "1  Since 2000, the winner of the Kate Greenaway m...                     7   \n",
      "2  Since 2000, the winner of the Kate Greenaway m...                     7   \n",
      "3  Since 2000, the winner of the Kate Greenaway m...                     7   \n",
      "4  Since 2000, the winner of the Kate Greenaway m...                     7   \n",
      "\n",
      "   aspect  worker_id  rating  \n",
      "0       0          7      55  \n",
      "1       0          5      59  \n",
      "2       1          5      27  \n",
      "3       1          3     100  \n",
      "4       2          8      36  \n",
      "\n",
      "Number of rows: 4500\n",
      "\n",
      "Number of columns: 6\n",
      "\n",
      "* Label distribution:\n",
      "Min: 0\n",
      "Max: 100\n",
      "Mean: 48.528444444444446\n",
      "Variance: 1561.4283020246912\n"
     ]
    }
   ],
   "source": [
    "print_data_basics(asset, label_column=\"rating\", json_column=\"full\", label_type=\"real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: IMDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/Users/margaretmitchell/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train = imdb['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Peek at data:\n",
      "                                                text  label\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
      "1  Homelessness (or Houselessness as George Carli...      1\n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
      "3  This is easily the most underrated film inn th...      1\n",
      "4  This is not the typical Mel Brooks film. It wa...      1\n",
      "\n",
      "Number of rows: 25000\n",
      "\n",
      "Number of columns: 2\n",
      "\n",
      "* Label distribution:\n",
      "1    12500\n",
      "0    12500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_data_basics(imdb_train, label_column=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for IMDB\n",
    "alllist = [cleanhtml(sent) for sent in imdb_train[\"text\"]]\n",
    "imdb_text = ' '. join(s for s in alllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 75949 words including stop words\n",
      "There are 75796 words after removing stop words\n",
      "There are 68187 words after removing stop words and lemmatizing\n"
     ]
    }
   ],
   "source": [
    "print_count_vocab(imdb_text, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity based on Wikipedia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using the pretrained model from CCNet https://github.com/facebookresearch/cc_net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=alllist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"en.sp.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hb/0w9fvsq54r9dk6qr52m0m5z40000gn/T/ipykernel_60351/974871058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en.sp.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mInit\u001b[0;34m(self, model_file, model_proto, out_type, add_bos, add_eos, reverse, enable_sampling, nbest_size, alpha)\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/lib/python3.9/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDecodeIdsWithCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"en.sp.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "sp_model = sentencepiece.SentencePieceProcessor('en.sp.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD. Issue with accessing kenlm.\n",
    "model= kenlm.Model('/home/sasha/Documents/MilaPostDoc/Python/cc_net/data/lm_sp/en.arpa.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=0\n",
    "doc_length=0\n",
    "for sentence in sent_tokenize(test):\n",
    "    sentence = sp_model.encode_as_pieces(sentence)\n",
    "    score += model.score(\" \".join(sentence))\n",
    "    doc_length += len(sentence) + 1\n",
    "print(\"Final score: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from  https://stackoverflow.com/questions/54941966/how-can-i-calculate-perplexity-using-nltk/55043954\n",
    "\n",
    "train_sentences = ['an apple', 'an orange']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in train_sentences]\n",
    "n = 1\n",
    "train_data, padded_vocab = padded_everygram_pipeline(n, tokenized_text)\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_vocab)\n",
    "\n",
    "test_sentences = ['an apple', 'an ant']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in test_sentences]\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_text)\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "for i, test in enumerate(test_data):\n",
    "    print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
