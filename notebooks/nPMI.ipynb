{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97024cf4-c930-4aa1-a13f-ecdcc2462afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5404d6b6-6ce1-48df-8999-7da3fb082662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be defined by the drop down in the UI\n",
    "subgroup1 = \"woman\"\n",
    "subgroup2 = \"man\"\n",
    "subgroup3 = \"non-binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bf08cc-e18f-49fc-bae7-f03a9a492baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"c4\", \"en\", split= \"train\", streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6ee78-6f96-45e2-8dad-ec745c953d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Just taking the first 10000 instances.\n"
     ]
    }
   ],
   "source": [
    "grab_n = 10000\n",
    "# For streaming data\n",
    "print('Note: Just taking the first %s instances.' % grab_n)\n",
    "data_head = data.take(grab_n)\n",
    "#data_head = [[\"there is a woman with a hairbrush\"],[\"there is a woman with a hairbrush\"],[\"there is a woman with a hairbrush\"],[\"there is a man with a dog\"],[\"there is a man with a dog\"]]\n",
    "df = pd.DataFrame(data_head, columns=[\"text\"])\n",
    "# If not streaming, use:\n",
    "#df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4d0c8-fc7c-4e99-8207-2e93d1c9b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vocab_frequencies(df):\n",
    "    \"\"\"\n",
    "    Based on an input pandas DataFrame with a 'text' column, \n",
    "    this function will count the occurrences of all words\n",
    "    with a frequency higher than 'cutoff' and will return another DataFrame\n",
    "    with the rows corresponding to the different vocabulary words\n",
    "    and the column to the count count of that word.\n",
    "    \"\"\"\n",
    "    # Move this up as a constant in larger code.\n",
    "    batch_size = 10\n",
    "    \n",
    "    # We do this to calculate per-word statistics\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    # Regex for pulling out single words\n",
    "    cvec = CountVectorizer(token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", lowercase=True)\n",
    "    \n",
    "    # We also do this because we need to have the tokenization per sentence \n",
    "    # so that we can look at co-occurrences of words across sentences for nPMI calculation\n",
    "    sent_tokenizer = cvec.build_tokenizer()\n",
    "    df['tokenized'] = df.text.apply(sent_tokenizer)\n",
    "    \n",
    "    # Fast calculation of single word counts\n",
    "    cvec.fit(df.text)\n",
    "    document_matrix = cvec.transform(df.text)\n",
    "    batches = np.linspace(0, df.shape[0], batch_size).astype(int)\n",
    "    i = 0\n",
    "    tf = []\n",
    "    while i < len(batches) - 1:\n",
    "        batch_result = np.sum(document_matrix[batches[i]:batches[i+1]].toarray(), axis=0)\n",
    "        tf.append(batch_result)\n",
    "        i += 1\n",
    "    term_freq_df = pd.DataFrame([np.sum(tf, axis=0)], columns=cvec.get_feature_names()).transpose()\n",
    "    \n",
    "    # Now organize everything into the dataframes\n",
    "    term_freq_df.columns = ['count']\n",
    "    term_freq_df.index.name = 'word'\n",
    "    sorted_term_freq_df = pd.DataFrame(term_freq_df.sort_values(by='count', ascending=False)['count'])\n",
    "    return sorted_term_freq_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b291a4-088e-463e-8a5e-326bcb0d9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df, df = count_vocab_frequencies(df)\n",
    "# p(word).  Note that multiple occurrences of a word in a sentence increases its probability.\n",
    "# We may want to do something about that.\n",
    "term_df['proportion'] = term_df['count']/float(sum(term_df['count']))\n",
    "# Sanity check\n",
    "print(term_df.head())\n",
    "print(term_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202c12e-6d02-4dbc-a896-c1ce1d3b34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PMI(df_coo, subgroup):\n",
    "    # PMI(x;y) = h(y) - h(y|x)\n",
    "    #          = h(subgroup) - h(subgroup|word)\n",
    "    #          = log (p(subgroup|word) / p(subgroup))\n",
    "    # nPMI additionally divides by -log(p(x,y)) = -log(p(x|y)p(y))\n",
    "    #\n",
    "    # Calculation of p(subgroup)\n",
    "    subgroup_prob = term_df.loc[subgroup]['proportion']\n",
    "    # Apply a function to all words to calculate log p(subgroup|word)\n",
    "    # The word is indexed by mlb.classes_ ; \n",
    "    # we pull out the word using the mlb.classes_ index and then get its count using our main term_df\n",
    "    # Calculation:\n",
    "    # p(subgroup|word) = count(subgroup,word) / count(word)\n",
    "    #                  = x.values             / term_df.loc[mlb.classes_[x.index]]['count']\n",
    "    pmi_df = pd.DataFrame(df_coo.apply(lambda x: np.log(x.values/term_df.loc[mlb.classes_[x.index]]['count']/subgroup_prob)))\n",
    "    pmi_df.columns = ['pmi']\n",
    "    # If all went well, this will be correlated with high frequency words\n",
    "    # Until normalizing\n",
    "    # Note: A potentially faster solution for adding count, npmi, can be based on this:\n",
    "    # #df_test['size_kb'],  df_test['size_mb'], df_test['size_gb'] = zip(*df_test['size'].apply(sizes))\n",
    "    return pmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6626f-ce6d-4630-a92b-ba89882743ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nPMI(pmi_df, df_coo):\n",
    "    normalize_df = pd.DataFrame(df_coo.apply(lambda x: -np.log(x.values/term_df.loc[mlb.classes_[x.index]]['count'] * term_df.loc[mlb.classes_[x.index]]['proportion'])))\n",
    "    # npmi_df = pmi_df/normalize_df\n",
    "    npmi_df = pd.DataFrame(pmi_df['pmi']/normalize_df[0])\n",
    "    npmi_df.columns = ['npmi']\n",
    "    return npmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4ba76-c3ad-4f4e-bdba-8b2653ff8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(df_coo, subgroup):\n",
    "    # TBH I have no clue why this works.\n",
    "    count_df = pd.DataFrame(df_coo.apply(lambda x: pd.Series(x.values, mlb.classes_[x.index])))\n",
    "    count_df.columns=['count']\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7207c16-7f2c-470a-b9d7-6a52e03b6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Makes a sparse vector (shape: # sentences x # words),\n",
    "# with the count of each word per sentence.\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_mlb = pd.DataFrame(mlb.fit_transform(df['tokenized']))\n",
    "\n",
    "# Calculates PMI metrics\n",
    "paired_results = pd.DataFrame()\n",
    "results_dict = {}\n",
    "for subgroup in (subgroup1, subgroup2):\n",
    "    # Index of the subgroup word in the sparse vector\n",
    "    subgroup_idx = np.where(mlb.classes_ == subgroup)[0][0]\n",
    "    # Dataframe for the subgroup (with counts)\n",
    "    df_subgroup = df_mlb.iloc[:, subgroup_idx]\n",
    "    # Create cooccurence matrix for the given subgroup and all other words.\n",
    "    # Note it also includes the word itself, so that count should maybe be subtracted \n",
    "    # (the word will always co-occur with itself)\n",
    "    print('Calculating co-occurrences')\n",
    "    df_coo = pd.DataFrame(df_mlb.T.dot(df_subgroup))#.drop(index=subgroup_idx, axis=1)\n",
    "    print('Getting counts for subgroup...')\n",
    "    count_df = get_count(df_coo, subgroup)\n",
    "    print(count_df)\n",
    "    print('Calculating PMI...')\n",
    "    pmi_df = get_PMI(df_coo, subgroup)\n",
    "    print(pmi_df)\n",
    "    print('Calculating nPMI...')\n",
    "    #pmi_df_pair[subgroup] = pmi_df\n",
    "    npmi_df = get_nPMI(pmi_df, df_coo)\n",
    "    print(npmi_df)\n",
    "    #results_df = pd.concat([count_df,pmi_df,npmi_df], axis=1)\n",
    "    paired_results[subgroup + '-pmi']  = pmi_df['pmi']\n",
    "    paired_results[subgroup + '-npmi'] = npmi_df['npmi']\n",
    "    paired_results[subgroup + '-count'] = count_df['count'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aea04d-09e6-4d0b-86c8-fed6fa26b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paired_results.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670174e-61b4-4af8-9203-540ec928dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# woman - man: If it's negative, it's man-biased; if it's positive, it's woman positive.\n",
    "npmi_bias = paired_results[subgroup1 + '-npmi'] - paired_results[subgroup2 + '-npmi'] #pd.DataFrame(results_dict[subgroup1]['npmi'] - results_dict[subgroup2]['npmi']).dropna()\n",
    "paired_results['npmi_bias'] = npmi_bias.dropna()\n",
    "paired_results = paired_results.dropna()\n",
    "#pmi_bias = pd.DataFrame(pmi_df_pair[subgroup1] - pmi_df_pair[subgroup2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bedd3c-6ae4-44e5-a063-80d943efbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd94085-e77e-4fe3-a955-1821f54426de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"@%s, the %s bias is:\\t%.2f\" % (n, subgroup2, np.abs(sum(paired_results.npmi_bias[:n].values))))\n",
    "print(\"@%s, the %s bias is:\\t%.2f\" % (n, subgroup1, sum(paired_results.npmi_bias[-n:].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec116a-29d5-4b88-8aac-cd3ced1aa8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top %s most %s-biased words\" % (n, subgroup2))\n",
    "paired_results.npmi_bias.sort_values(ascending=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb965a-4709-4d03-b682-bc8c1af41175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top %s most %s-biased words\" % (n,subgroup1))\n",
    "paired_results.npmi_bias.sort_values(ascending=True)[-n:].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef0f47-2990-4991-a26e-714421f63f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paired_results.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803d76b-56a5-4b67-b8f4-8df204613d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paired_results.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
